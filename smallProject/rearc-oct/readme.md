### Part 0: prepare AWS programming locally

![image](https://github.com/superdba111/DataStructure/assets/31944577/0d7ccbc7-186a-41b3-8283-807bdfe3d404)

### Part 1: Sourcing the data

Run bls.py locally
![image](https://github.com/superdba111/DataStructure/assets/31944577/770c75f8-5b08-4017-8bfd-6694f7e5fec1)

### Part 2: APIS

run pop-s3.py locally
![image](https://github.com/superdba111/DataStructure/assets/31944577/0ade3f28-624a-4958-b7d8-64380e6059b1)

### Part 3: Data Analytics

Run rearc-oct notebook locally

![image](https://github.com/superdba111/DataStructure/assets/31944577/77f4cfc2-568d-4021-87a7-bb043a37e48b)


### Part 4: Iac & Data Pipeline 

loaddata lambda function using zip package layer
![image](https://github.com/superdba111/DataStructure/assets/31944577/05250cb4-6372-47ea-ac51-3e7f2ab95935)

reports using docker images

![image](https://github.com/superdba111/DataStructure/assets/31944577/0215bb5d-67d0-4c65-88e5-df002ec4bca1)

in cloudWatch, you can see the logging

![image](https://github.com/superdba111/DataStructure/assets/31944577/f6929599-544e-4cec-be34-8b0499073f0e)

Run terraform and created all resources and data pipeline (two lambda functions)

![image](https://github.com/superdba111/DataStructure/assets/31944577/09f8ceb6-f5f3-4113-a2ee-be47b9164609)
![image](https://github.com/superdba111/DataStructure/assets/31944577/90c55ba3-14c0-49ad-9adf-ac519f8cd55e)










